name: Scrape Tweets to Supabase

on:
  # run twice per day
  schedule:
    # ~9:00 AM New York (13:00 UTC summer, 14:00 UTC winter)
    - cron: '0 13 * * *'
    # ~3:30 PM New York (19:30 UTC summer, 20:30 UTC winter)
    - cron: '30 19 * * *'
  # manual trigger from the Actions tab
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      # 1️⃣  Checkout repository
      - uses: actions/checkout@v4

      # 2️⃣  Install R -------------------------------------------------
      - uses: r-lib/actions/setup-r@v2
        with:
          r-version: '4.3'

      # 3️⃣  System libraries for R packages that need compilation
      - name: Install system deps
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            build-essential libpq-dev libssl-dev \
            libcurl4-openssl-dev libxml2-dev \
            python3-dev python3-venv libpng-dev zlib1g-dev

      # 4️⃣  Python for reticulate / twscrape --------------------------
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # 5️⃣  Run the R script -----------------------------------------
      - name: Run twitter_ingest.R
        env:
          # ---- Supabase --------------------------------------------
          SUPABASE_HOST:   ${{ secrets.SUPABASE_HOST }}
          SUPABASE_PORT:   ${{ secrets.SUPABASE_PORT }}
          SUPABASE_DB:     ${{ secrets.SUPABASE_DB }}
          SUPABASE_USER:   ${{ secrets.SUPABASE_USER }}
          SUPABASE_PWD:    ${{ secrets.SUPABASE_PWD }}
          # ---- Twitter cookies -------------------------------------
          TW_COOKIES_JSON: ${{ secrets.TW_COOKIES_JSON }}
          # optional: comma-separated list of handles
          TW_HANDLES:      ${{ vars.TW_HANDLES }}
          # where the Python virtual-env will live on the runner
          PY_VENV_PATH:    ".venv"
        run: |
          Rscript twitter_ingest.R
